{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "03-writing-prompt-reward-model-training",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anshradh/trl_custom/blob/test/03_writing_prompt_reward_model_training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Writing Prompt Reward Model Training\n",
        "The idea is to train a model to predict which response to a reddit writing prompt was ranked the highest, which we'll use as a reward model for training a LM to output human-preferred responses to reddit writing prompts."
      ],
      "metadata": {
        "id": "TI4f6x7ibTlL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Prerequisites"
      ],
      "metadata": {
        "id": "ec62dWuscC3c"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MWsii3yVkeBa"
      },
      "outputs": [],
      "source": [
        "# Install needed libraries and log into huggingface\n",
        "!pip install datasets\n",
        "!pip install transformers\n",
        "!pip install accelerate\n",
        "!pip install huggingface_hub\n",
        "!apt install git-lfs\n",
        "from huggingface_hub import notebook_login\n",
        "notebook_login()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from tqdm.auto import tqdm\n",
        "import numpy as np\n",
        "from torch.utils.data import DataLoader\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch.nn.functional as F\n",
        "from torch.optim import Adam\n",
        "import torch\n",
        "import collections\n",
        "import random\n",
        "tqdm.pandas()\n",
        "\n",
        "from datasets import load_dataset, ClassLabel, load_metric, concatenate_datasets\n",
        "\n",
        "from transformers import AutoModel, AutoTokenizer\n",
        "from transformers import top_k_top_p_filtering\n",
        "from torch import nn\n",
        "from torch.nn import Identity\n",
        "import torch.nn.functional as F\n",
        "import torch\n",
        "\n",
        "from transformers import AutoModelForSequenceClassification, DataCollatorWithPadding, AdamW, get_scheduler\n",
        "\n",
        "from accelerate import Accelerator"
      ],
      "metadata": {
        "id": "lf_Mh741k5Du"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Preprocessing"
      ],
      "metadata": {
        "id": "igAkQPDwd3Tt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load dataset from huggingface\n",
        "prompt_response_dataset = load_dataset(\"rewardsignal/reddit_writing_prompts\", data_files=\"prompt_responses_full.csv\", split='train[:80%]')"
      ],
      "metadata": {
        "id": "4kYQTxa1lBPa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## We tokenize and preprocess the text portion of the dataset here\n",
        "# tokenizer_name = input()\n",
        "tokenizer_name = 'distilgpt2'\n",
        "tokenizer = AutoTokenizer.from_pretrained(tokenizer_name, use_fast=True)\n",
        "prompt_prefix = \"Writing Prompt: \"\n",
        "response_prefix = \"Response: \"\n",
        "\n",
        "def preprocess_text_function(examples):\n",
        "  examples[\"prompt\"] = [prompt.replace('[WP] ', prompt_prefix) for prompt in examples[\"prompt\"]]\n",
        "  examples[\"response\"] = [response_prefix + response for response in examples[\"response\"]]\n",
        "  return tokenizer(examples['prompt'], examples['response'], truncation=True)\n",
        "\n",
        "tokenized_reward_dataset = prompt_response_dataset.map(preprocess_text_function, batched=True, num_proc=4)"
      ],
      "metadata": {
        "id": "P0_xIfMFlB2f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Here we binarize the labels (best-ranked response receives a label of 1, rest get a label of 0) and remove extraneous dataset columns\n",
        "def preprocess_labels_function(examples):\n",
        "  examples['labels'] = [1 if (rank == 0) else 0 for rank in examples[\"response_rank\"]]\n",
        "  return examples\n",
        "tokenized_reward_dataset = tokenized_reward_dataset.map(preprocess_labels_function, batched=True, num_proc=4)\n",
        "tokenized_reward_dataset.cast_column(\"labels\", ClassLabel(num_classes=2, names=['not-best', 'best'], names_file=None, id=None))\n",
        "tokenized_reward_dataset = tokenized_reward_dataset.remove_columns(['Unnamed: 0', 'prompt_id', 'prompt', 'prompt_score', 'prompt_created_utc', 'response_id', 'response', 'response_score', 'response_created_utc', 'num_responses', 'response_children', 'score_bin', 'response_rank']\n",
        ")\n",
        "tokenized_reward_dataset.set_format(\"torch\")"
      ],
      "metadata": {
        "id": "L7gxsrXkcsrA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Balance our dataset (only select a small portion of the \"not-best\" labeled examples to match the number of best writing response examples)\n",
        "positive_reward_dataset = tokenized_reward_dataset.filter(lambda example: example['labels'] == 1)\n",
        "negative_reward_dataset = tokenized_reward_dataset.filter(lambda example: example['labels'] == 0).shuffle(seed=42).select(range(len(positive_reward_dataset)))\n",
        "tokenized_reward_dataset = concatenate_datasets([positive_reward_dataset, negative_reward_dataset])"
      ],
      "metadata": {
        "id": "oYnjHO79lHy-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Getting ready for training"
      ],
      "metadata": {
        "id": "5LiKWRYbd7iC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Split into training and evaluation datasets\n",
        "reward_train_dataset = tokenized_reward_dataset.shuffle(seed=42).select(range(4*len(tokenized_reward_dataset)//5))\n",
        "reward_eval_dataset = tokenized_reward_dataset.shuffle(seed=42).select(range(4*len(tokenized_reward_dataset)//5, len(tokenized_reward_dataset)))"
      ],
      "metadata": {
        "id": "rpRPOtpflJ0q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Set up dataloaders for training and evaluating, as well as other essentials for running the training loop\n",
        "# reward_model_name = input()\n",
        "reward_model_name = 'distilgpt2'\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
        "## Load pre-trained sequence classification model\n",
        "reward_model = AutoModelForSequenceClassification.from_pretrained(reward_model_name, num_labels = 2)\n",
        "reward_model.config.pad_token_id = reward_model.config.eos_token_id\n",
        "\n",
        "train_dataloader = DataLoader(\n",
        "    reward_train_dataset, shuffle=True, batch_size=4, collate_fn=data_collator\n",
        ")\n",
        "eval_dataloader = DataLoader(\n",
        "    reward_eval_dataset, batch_size=4, collate_fn=data_collator\n",
        ")\n",
        "\n",
        "optimizer = AdamW(reward_model.parameters(), lr=3e-5)\n",
        "accelerator = Accelerator()\n",
        "train_dataloader, eval_dataloader, reward_model, optimizer = accelerator.prepare(train_dataloader, eval_dataloader, reward_model, optimizer)\n",
        "num_epochs = 5\n",
        "num_training_steps = num_epochs * len(train_dataloader)\n",
        "lr_scheduler = get_scheduler(\n",
        "    \"linear\",\n",
        "    optimizer=optimizer,\n",
        "    num_warmup_steps=0,\n",
        "    num_training_steps=num_training_steps,\n",
        ")\n",
        "\n",
        "progress_bar = tqdm(range(num_training_steps))"
      ],
      "metadata": {
        "id": "BFnSkW7-lQFu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training"
      ],
      "metadata": {
        "id": "VQnT-mkld_0Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Run training loop for the reward model\n",
        "reward_model.train()\n",
        "for epoch in range(num_epochs):\n",
        "    for batch in train_dataloader:\n",
        "        outputs = reward_model(**batch)\n",
        "        loss = outputs.loss\n",
        "        accelerator.backward(loss)\n",
        "        optimizer.step()\n",
        "        lr_scheduler.step()\n",
        "        optimizer.zero_grad()\n",
        "        progress_bar.update(1)"
      ],
      "metadata": {
        "id": "ciTOTpc0lVRx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluation"
      ],
      "metadata": {
        "id": "AoW4QYm0eDPi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Evaluate accuracy of the reward model on the evaluation dataset\n",
        "metric = load_metric(\"accuracy\")\n",
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "reward_model.to(device)\n",
        "reward_model.eval()\n",
        "count = 0\n",
        "for batch in eval_dataloader:\n",
        "    count += 1\n",
        "    batch = {k: v.to(device) for k, v in batch.items()}\n",
        "    with torch.no_grad():\n",
        "        outputs = reward_model(**batch)\n",
        "\n",
        "    logits = outputs.logits\n",
        "    predictions = torch.argmax(logits, dim=-1)\n",
        "    metric.add_batch(predictions=predictions, references=batch[\"labels\"])\n",
        "\n",
        "metric.compute()"
      ],
      "metadata": {
        "id": "bE9W05ADlV7c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Push the model to the hugginface hub\n",
        "reward_model.push_to_hub(model_name + \"_reward_model\", use_temp_dir=True)"
      ],
      "metadata": {
        "id": "RMaBWvqX8mlw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Results and Discussion\n"
      ],
      "metadata": {
        "id": "NTWc3p95dzYr"
      }
    }
  ]
}